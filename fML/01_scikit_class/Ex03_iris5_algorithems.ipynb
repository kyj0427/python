{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 최적의 알고리즘 찾기\n",
    "\n",
    "    - all_estimators() 메소드 이용하여 모든 알고리즘 추출\n",
    "    \n",
    "    [참고도서] 파이썬을 이용한 머신러닝, 딥러닝 실전 앱 개발 (위키북스)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import all_estimators\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 붓꽃 데이터 읽어 들이기\n",
    "iris_data = pd.read_csv(\"../data/iris/iris.csv\", encoding=\"utf-8\")\n",
    "\n",
    "# 붓꽃 데이터를 레이블과 입력 데이터로 분리하기 \n",
    "y = iris_data.loc[:,\"variety\"]\n",
    "x = iris_data.loc[:,[\"sepal.length\",\"sepal.width\",\"petal.length\",\"petal.width\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier 의 정답률 =  0.9333333333333333\n",
      "BaggingClassifier 의 정답률 =  0.9666666666666667\n",
      "BernoulliNB 의 정답률 =  0.23333333333333334\n",
      "CalibratedClassifierCV 의 정답률 =  0.9333333333333333\n",
      "CategoricalNB 의 정답률 =  0.9\n",
      "ComplementNB 의 정답률 =  0.6666666666666666\n",
      "DecisionTreeClassifier 의 정답률 =  0.9666666666666667\n",
      "DummyClassifier 의 정답률 =  0.23333333333333334\n",
      "ExtraTreeClassifier 의 정답률 =  0.9333333333333333\n",
      "ExtraTreesClassifier 의 정답률 =  0.9333333333333333\n",
      "GaussianNB 의 정답률 =  0.9666666666666667\n",
      "GaussianProcessClassifier 의 정답률 =  0.9333333333333333\n",
      "GradientBoostingClassifier 의 정답률 =  0.9333333333333333\n",
      "HistGradientBoostingClassifier 의 정답률 =  0.9333333333333333\n",
      "KNeighborsClassifier 의 정답률 =  1.0\n",
      "LabelPropagation 의 정답률 =  0.9333333333333333\n",
      "LabelSpreading 의 정답률 =  0.9333333333333333\n",
      "LinearDiscriminantAnalysis 의 정답률 =  1.0\n",
      "LinearSVC 의 정답률 =  0.9333333333333333\n",
      "LogisticRegression 의 정답률 =  0.9333333333333333\n",
      "LogisticRegressionCV 의 정답률 =  1.0\n",
      "MLPClassifier 의 정답률 =  0.9666666666666667\n",
      "MultinomialNB 의 정답률 =  0.7666666666666667\n",
      "NearestCentroid 의 정답률 =  0.9333333333333333\n",
      "NuSVC 의 정답률 =  0.9333333333333333\n",
      "PassiveAggressiveClassifier 의 정답률 =  0.7666666666666667\n",
      "Perceptron 의 정답률 =  0.6666666666666666\n",
      "QuadraticDiscriminantAnalysis 의 정답률 =  1.0\n",
      "RadiusNeighborsClassifier 의 정답률 =  0.9333333333333333\n",
      "RandomForestClassifier 의 정답률 =  0.9666666666666667\n",
      "RidgeClassifier 의 정답률 =  0.8333333333333334\n",
      "RidgeClassifierCV 의 정답률 =  0.8333333333333334\n",
      "SGDClassifier 의 정답률 =  0.9666666666666667\n",
      "SVC 의 정답률 =  0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# (0) 학습 전용과 테스트 전용 분리하기 \n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, train_size = 0.8, shuffle = True)\n",
    "\n",
    "# (1) classifier 알고리즘 모두 추출하기\n",
    "allAlgorithms = all_estimators(type_filter=\"classifier\")\n",
    "\n",
    "for(name, algorithm) in allAlgorithms:\n",
    "    # (1) 알고리즘 종류 확인\n",
    "    # print(name)\n",
    "    \n",
    "    # 현재 자료형이 안 맞아서 학습되지 않는 알고리즘들이기에 제외해야 한다 -> 추가로 코딩을 예쁘게 \n",
    "    if name=='CheckingClassifier' or name=='ClassifierChain' \\\n",
    "        or name=='MultiOutputClassifier' or name=='OneVsOneClassifier' \\\n",
    "        or name=='OneVsRestClassifier' or name=='OutputCodeClassifier' \\\n",
    "        or name=='FixedThresholdClassifier' or name=='TunedThresholdClassifierCV'\\\n",
    "        or name=='VotingClassifier' or name=='StackingClassifier':\n",
    "        continue\n",
    "        \n",
    "    #-----------------------------------------    \n",
    "    # 에러가 발생하면 위에 print(name)으로 알고리즘을 확인 후 에러난 알고리즘이름을 위에 추가한다\n",
    "    \n",
    "    # (2) 각 알고리즘 객체 생성하기 \n",
    "    clf = algorithm()\n",
    "\n",
    "    # (3) 학습하고 평가하기 \n",
    "    clf.fit(x_train, y_train)\n",
    "    y_pred = clf.predict(x_test)\n",
    "    print(name,\"의 정답률 = \" , accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ 문제점 ]\n",
    "\n",
    "각 알고리즘의 정답률을 비교했지만, 평가 횟수가 1회 밖에고 사용한 데이타도 한 가지 패턴이다.\n",
    "\n",
    "여러 데이타 패턴으로 평가하는 방법이 교차 검증(corss-validation)이다\n",
    "\n",
    "\n",
    "### K-교차 검증 (cross-validataion)\n",
    "\n",
    "- K개의 그룹으로 분할\n",
    "- K-1개의 학습 데이타\n",
    "- 남은 1개의 평가 데이타\n",
    "- K번 반복\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.utils import all_estimators\n",
    "from sklearn.model_selection import KFold\n",
    "import warnings\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# 붓꽃 데이터 읽어 들이기\n",
    "iris_data = pd.read_csv(\"../data/iris/iris.csv\", encoding=\"utf-8\")\n",
    "\n",
    "# 붓꽃 데이터를 레이블과 입력 데이터로 분리하기 \n",
    "y = iris_data.loc[:,\"variety\"]\n",
    "x = iris_data.loc[:,[\"sepal.length\",\"sepal.width\",\"petal.length\",\"petal.width\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier 의 정답률=\n",
      "[0.9        0.96666667 0.93333333 1.         0.93333333]\n",
      "BaggingClassifier 의 정답률=\n",
      "[0.96666667 0.93333333 0.96666667 0.93333333 0.93333333]\n",
      "BernoulliNB 의 정답률=\n",
      "[0.16666667 0.16666667 0.3        0.26666667 0.26666667]\n",
      "CalibratedClassifierCV 의 정답률=\n",
      "[0.96666667 0.83333333 0.96666667 0.86666667 0.93333333]\n",
      "CategoricalNB 의 정답률=\n",
      "[0.96666667 0.83333333 0.93333333 1.         0.9       ]\n",
      "ComplementNB 의 정답률=\n",
      "[0.7        0.56666667 0.8        0.53333333 0.73333333]\n",
      "DecisionTreeClassifier 의 정답률=\n",
      "[1.         0.9        0.9        0.96666667 0.86666667]\n",
      "DummyClassifier 의 정답률=\n",
      "[0.3        0.23333333 0.2        0.2        0.3       ]\n",
      "ExtraTreeClassifier 의 정답률=\n",
      "[0.9        0.9        0.96666667 1.         0.96666667]\n",
      "ExtraTreesClassifier 의 정답률=\n",
      "[0.93333333 0.96666667 0.93333333 1.         0.96666667]\n",
      "GaussianNB 의 정답률=\n",
      "[1.         0.93333333 0.96666667 0.93333333 0.93333333]\n",
      "GaussianProcessClassifier 의 정답률=\n",
      "[0.96666667 0.86666667 0.96666667 0.96666667 0.96666667]\n",
      "GradientBoostingClassifier 의 정답률=\n",
      "[0.96666667 0.93333333 0.93333333 0.96666667 0.93333333]\n",
      "HistGradientBoostingClassifier 의 정답률=\n",
      "[0.96666667 0.93333333 0.93333333 0.96666667 0.93333333]\n",
      "KNeighborsClassifier 의 정답률=\n",
      "[0.96666667 0.96666667 0.93333333 0.86666667 1.        ]\n",
      "LabelPropagation 의 정답률=\n",
      "[0.9        0.96666667 0.96666667 0.96666667 0.93333333]\n",
      "LabelSpreading 의 정답률=\n",
      "[0.96666667 0.93333333 1.         0.93333333 0.96666667]\n",
      "LinearDiscriminantAnalysis 의 정답률=\n",
      "[0.96666667 1.         1.         0.96666667 0.96666667]\n",
      "LinearSVC 의 정답률=\n",
      "[0.93333333 0.93333333 0.93333333 0.96666667 0.96666667]\n",
      "LogisticRegression 의 정답률=\n",
      "[0.96666667 0.96666667 1.         0.96666667 0.93333333]\n",
      "LogisticRegressionCV 의 정답률=\n",
      "[0.93333333 0.93333333 0.96666667 0.96666667 0.96666667]\n",
      "MLPClassifier 의 정답률=\n",
      "[0.9        1.         1.         1.         0.96666667]\n",
      "MultinomialNB 의 정답률=\n",
      "[0.7 0.9 0.9 1.  0.9]\n",
      "NearestCentroid 의 정답률=\n",
      "[0.9        0.93333333 0.9        0.93333333 0.93333333]\n",
      "NuSVC 의 정답률=\n",
      "[0.9        1.         0.93333333 0.9        1.        ]\n",
      "PassiveAggressiveClassifier 의 정답률=\n",
      "[0.9        0.86666667 0.76666667 0.93333333 1.        ]\n",
      "Perceptron 의 정답률=\n",
      "[0.86666667 0.93333333 0.9        0.7        0.36666667]\n",
      "QuadraticDiscriminantAnalysis 의 정답률=\n",
      "[1.         0.93333333 0.96666667 0.96666667 1.        ]\n",
      "RadiusNeighborsClassifier 의 정답률=\n",
      "[1.         0.93333333 0.96666667 0.96666667 0.96666667]\n",
      "RandomForestClassifier 의 정답률=\n",
      "[0.96666667 0.9        1.         0.96666667 0.96666667]\n",
      "RidgeClassifier 의 정답률=\n",
      "[0.86666667 0.93333333 0.8        0.9        0.7       ]\n",
      "RidgeClassifierCV 의 정답률=\n",
      "[0.93333333 0.83333333 0.8        0.76666667 0.9       ]\n",
      "SGDClassifier 의 정답률=\n",
      "[1.         0.93333333 0.36666667 0.76666667 0.83333333]\n",
      "SVC 의 정답률=\n",
      "[0.93333333 0.96666667 1.         1.         0.93333333]\n"
     ]
    }
   ],
   "source": [
    "# classifier 알고리즘 모두 추출하기\n",
    "warnings.filterwarnings('ignore')\n",
    "allAlgorithms = all_estimators(type_filter=\"classifier\")\n",
    "\n",
    "# (1)\n",
    "# K-분할 크로스 밸리데이션 전용 객체 \n",
    "# 데이타를 5그룹으로 분할하고 분할할 때 랜덤하게 섞는다\n",
    "kfold_cv = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "for(name, algorithm) in allAlgorithms:\n",
    "    \n",
    "    # 현재 자료형이 안 맞아서 학습되지 않는 알고리즘들이기에 제외해야 한다 -> 추가로 코딩을 예쁘게 \n",
    "    if name=='CheckingClassifier' or name=='ClassifierChain' \\\n",
    "        or name=='MultiOutputClassifier' or name=='OneVsOneClassifier' \\\n",
    "        or name=='OneVsRestClassifier' or name=='OutputCodeClassifier' \\\n",
    "        or name=='FixedThresholdClassifier' or name=='TunedThresholdClassifierCV'\\\n",
    "        or name=='VotingClassifier' or name=='StackingClassifier':\n",
    "        continue\n",
    "        \n",
    "    # (2) 각 알고리즘 객체 생성하기\n",
    "    clf = algorithm()\n",
    "\n",
    "    # (3)\n",
    "    # score 메서드를 가진 클래스를 대상으로 하기\n",
    "    if hasattr(clf,\"score\"):\n",
    "\n",
    "        # (4) 크로스 밸리데이션\n",
    "        # cross_val_score (clf:알고리즘 객체, x:입력데이터, y:레이블데이터, cv:교차검증 적용 객체)\n",
    "        scores = cross_val_score(clf, x, y, cv=kfold_cv)\n",
    "        print(name,\"의 정답률=\")\n",
    "        print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 최적의 매개변수 찾기\n",
    "\n",
    "위에서는 각각의 알고리즘 객체에 디폴트 매개변수만 사용하여 결과를 확인\n",
    "\n",
    "그러나 매개변수를 변경하면 결과가 달라지게\n",
    "\n",
    "최적의 매개변수를 찾는 것도 중요하다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.utils import all_estimators\n",
    "from sklearn.model_selection import KFold\n",
    "import warnings\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# 붓꽃 데이터 읽어 들이기\n",
    "iris_data = pd.read_csv(\"../data/iris/iris.csv\", encoding=\"utf-8\")\n",
    "\n",
    "# 붓꽃 데이터를 레이블과 입력 데이터로 분리하기 \n",
    "y = iris_data.loc[:,\"variety\"]\n",
    "x = iris_data.loc[:,[\"sepal.length\",\"sepal.width\",\"petal.length\",\"petal.width\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최적의 매개 변수 =  SVC(C=1, kernel='linear')\n",
      "최종 정답률 =  1.0\n"
     ]
    }
   ],
   "source": [
    "# 학습 전용과 테스트 전용 분리하기 \n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, train_size = 0.8, shuffle = True)\n",
    "\n",
    "# 그리드 서치에서 사용할 매개 변수 --- (*1)\n",
    "parameters = [\n",
    "    {\"C\": [1, 10, 100, 1000], \"kernel\":[\"linear\"]},\n",
    "    {\"C\": [1, 10, 100, 1000], \"kernel\":[\"rbf\"], \"gamma\":[0.001, 0.0001]},\n",
    "    {\"C\": [1, 10, 100, 1000], \"kernel\":[\"sigmoid\"], \"gamma\": [0.001, 0.0001]}\n",
    "]\n",
    "\n",
    "# 그리드 서치 --- (*2)\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "kfold_cv = KFold(n_splits=5, shuffle=True)\n",
    "clf = GridSearchCV( SVC(), parameters, cv=kfold_cv)\n",
    "clf.fit(x_train, y_train)\n",
    "print(\"최적의 매개 변수 = \", clf.best_estimator_)\n",
    "\n",
    "# 최적의 매개 변수로 평가하기 --- (*3)\n",
    "y_pred = clf.predict(x_test)\n",
    "print(\"최종 정답률 = \" , accuracy_score(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
